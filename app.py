import streamlit as st
from llm_api import call_llm, call_llm_docs, classify_intent,handle_order_query
from vector_store import process_document_deepseek
from function import check_order_status

# Custom CSS styles
st.markdown("""
<style>
    .main {
        padding: 0rem 1rem;
    }
    .stSidebar {
        background-color: #f5f5f5;
    }
    .stButton>button {
        width: 100%;
        margin-bottom: 10px;
    }
    .message-container {
        display: flex;
        margin-bottom: 10px;
    }
    .user-message {
        background-color: #e1f5fe;
        padding: 10px;
        border-radius: 10px;
        margin-left: auto;
        max-width: 80%;
    }
    .bot-message {
        background-color: #f5f5f5;
        padding: 10px;
        border-radius: 10px;
        margin-right: auto;
        max-width: 80%;
    }
</style>
""", unsafe_allow_html=True)


def init_session_state():
    """Initialize variables"""
    if 'current_page' not in st.session_state:
        st.session_state.current_page = None
    
    #LLM Configuration
    if 'llm_config' not in st.session_state:
        st.session_state.llm_config ={
            'url':'https://api.deepseek.com',
            'api_key':'',
            'model':'deepseek-chat'
        }
    #Customer Assistant Configuration
    if 'bot_config' not in st.session_state:
        st.session_state.bot_config= {
            'name':'AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ',
            'description' : 'ç§ã¯AIãƒãƒ£ãƒƒãƒˆå›ã§ã™ï¼ãŠå®¢æ§˜ã‹ã‚‰ã®è³ªå•ã«ä¸å¯§ã«ãŠç­”ãˆã—ã¾ã™ï¼',
            'model':'deepseek-chat'
        }

    #Chat Message Information
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    #Knowledge base setting
    if 'knowledge_base' not in st.session_state:
        st.session_state.knowledge_base = {
            'chunks':[],
            'vector_store':None
        }    

    #Define orders
    if 'orders' not in st.session_state:
        st.session_state.orders = []    

def show_sidebar():
    """Show left sidebar"""
    with st.sidebar:
        st.title("AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ")
        st.markdown("---")

        #Menu button
        if st.button("ä¼šè©±ãƒãƒ£ãƒƒãƒˆ"):
            st.session_state.current_page = 'chat'

        #LLM Configuration
        if st.button("è¨€èªãƒ¢ãƒ‡ãƒ«è¨­å®š"):
            st.session_state.current_page = 'model_config'

        #Customer Assistant Configuration
        if st.button("AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆè¨­å®š"):
            st.session_state.current_page = 'bot_config'

        #Knowledge base configuration
        if st.button("ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹è¨­å®š"):
            st.session_state.current_page = 'knowledge_cofig'    

        #Loading order information
        if st.button("æ³¨æ–‡è¨­å®š"):
            st.session_state.current_page = "order_config"

def show_main_content():
    """Show main window"""

    if st.session_state.current_page == 'chat':
        show_chat()
    elif st.session_state.current_page == 'model_config':
        show_model_config()
    elif st.session_state.current_page == 'bot_config':
        show_bot_config()
    elif st.session_state.current_page == 'knowledge_cofig':
        show_knowledge_config()    
    else:
        st.title("AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¸ã‚ˆã†ã“ã")
        st.write("å·¦å´ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰æ©Ÿèƒ½ã‚’é¸æŠã—ã¦ãã ã•ã„")

def show_chat():
    """Show chat interface"""
    st.title("AIãƒãƒ£ãƒƒãƒˆå›")

    #Displays information about the message
    if not st.session_state.messages:
        st.session_state.messages.append({
            "role":"asssitant",
            #LLM Customer Service Assistant
            "content":st.session_state.bot_config['description']
        })
    
    #Create a dialog box to save the content of the question and answer, including the user's questions and the customer service assistant's answers
    chat_container = st.container()
    with chat_container:
        for message in st.session_state.messages:
            if message["role"] == "user":
                #Set the user-related message style, you can use markdown format
                st.markdown(f'<div class="message-container"><div class="user-message">{message["content"]}</div></div>', 
                           unsafe_allow_html=True)
            else:
                if message.get("is_knowledge"):
                    st.markdown(f'<div class="message-container"><div class="knowledge-message">{message["content"]}</div></div>', 
                               unsafe_allow_html=True)
                    with st.expander("ä¸€è‡´ã™ã‚‹ãƒŠãƒ¬ãƒƒã‚¸ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¡¨ç¤º"):
                        #Put the matching documents into docs
                        for i, doc in enumerate(message["docs"],1 ):
                            st.write(f"### ãƒŠãƒ¬ãƒƒã‚¸ãƒ–ãƒ­ãƒƒã‚¯{i}")
                            st.text(doc["page_content"])
                else:
                    #Set the AI assistant-related message style, you can use markdown format
                    st.markdown(f'<div class="message-container"><div class="bot-message">{message["content"]}</div></div>', 
                            unsafe_allow_html=True)
                    if message.get("sources"):
                        with st.expander("ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹å…ƒ"):
                            #Put the knowledge base source in sources
                            for source in message["sources"]:
                                st.write(f"{source}")
    
    message = st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    if message :
        #Here you need to call the LLM to process the message
        process_message(message)
        st.rerun()

def process_message(message: str):
    """Processing new messages"""
    if message.strip():
        intent = classify_intent(
            url=st.session_state.llm_config['url'],
            api_key=st.session_state.llm_config['api_key'],
            model_name=st.session_state.llm_config['model'],
            message= message,
            role=st.session_state.bot_config['description']
        )

        st.session_state.messages.append({
            "role":"assistant",
            "content":message,
            "sources": intent
        })

        # Routing to different processing flows based on intent type
        if intent['intent_type'] == 'order':
            response = handle_order_query(
                url=st.session_state.llm_config['url'],
                api_key=st.session_state.llm_config['api_key'],
                model_name=st.session_state.llm_config['model'],
                message=message,
                role=st.session_state.bot_config['description']
            )
        elif intent['intent_type'] == 'knowledge':
            response = handle_knowledge_query(message)
        else:  # Others, it will be handled by human
            response = handle_human_transfer(intent)

        
        # Add assistant reply
        st.session_state.messages.append({
            "role": "assistant",
            "content": response,
            "intent": intent  # Save intent information for debugging
        })

def handle_knowledge_query(message: str) -> str:
    """Handling knowledge base related issues"""
    context = ""
    sources = []
    
    # Check if the knowledge base is loaded
    if st.session_state.knowledge_base['vector_store'] is None:
        st.session_state.messages.append({
            "role": "assistant",
            "content": "âš ï¸ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã¯ã¾ã èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã¾ãšã¯ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹è¨­å®šãƒšãƒ¼ã‚¸ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
            "is_knowledge": False
        })
        return "ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã¯ã¾ã èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã¾ãšã¯ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹è¨­å®šãƒšãƒ¼ã‚¸ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
            
    retriever = st.session_state.knowledge_base['vector_store'].as_retriever()
    docs = retriever.invoke(message)
    context = "\n".join([doc.page_content for doc in docs])
    sources = [f"ãƒŠãƒ¬ãƒƒã‚¸ãƒ–ãƒ­ãƒƒã‚¯ #{i+1} " 
                for i, doc in enumerate(docs)]
    
    # Display knowledge base search results
    if docs:
        st.session_state.messages.append({
            "role": "assistant", 
            "content": "ğŸ” ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ä»¥ä¸‹ã®æƒ…å ±ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„:",
            "sources": sources,
            "is_knowledge": True,
            "docs": [{"page_content": doc.page_content, "metadata": doc.metadata} for doc in docs]
        })
    else:
        st.session_state.messages.append({
            "role": "assistant",
            "content": "â„¹ï¸ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã§é–¢é€£ã™ã‚‹ä¸€è‡´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
            "is_knowledge": False
        })

    if docs:  # If the knowledge base is hit
        bot_response = call_llm_docs(
            docs, 
            message,
            url=st.session_state.llm_config['url'],
            api_key=st.session_state.llm_config['api_key'],
            model_name=st.session_state.llm_config['model']
        )
    else:  # If the knowledge base is not hit
        system_prompt = f"{st.session_state.bot_config['description']}"
        if context:
            system_prompt += f"\n\nç¾åœ¨ã®ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context}"
        bot_response = call_llm(
            url=st.session_state.llm_config['url'],
            api_key=st.session_state.llm_config['api_key'],
            model_name=st.session_state.llm_config['model'],
            prompt=message,
            system_prompt=system_prompt
        )
    return bot_response

def handle_human_transfer(intent: dict) -> str:
    """Route to human customer service"""
    try:
        confidence = intent.get('confidence', 0.0)
        if confidence > 0.6:
            return "ã”è³ªå•ã«ã¯æ‰‹å‹•ã§ã®ã‚µãƒãƒ¼ãƒˆãŒå¿…è¦ã§ã™ã€‚ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒ¼ãƒ“ã‚¹æ‹…å½“è€…ã«è»¢é€ã„ãŸã—ã¾ã™..."
        return "ãŠå®¢æ§˜ã®å•é¡Œã¯æ‰‹å‹•ã§ã®å‡¦ç†ãŒå¿…è¦ã§ã™ã€‚ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒ¼ãƒ“ã‚¹ãƒã‚±ãƒƒãƒˆã‚’é€ä¿¡ã„ãŸã—ã¾ã—ãŸã®ã§ã€1æ™‚é–“ä»¥å†…ã«ã”é€£çµ¡ã„ãŸã—ã¾ã™ã€‚"
    except Exception as e:
        print(f"æ‰‹å‹•ã‚¨ãƒ©ãƒ¼å‡¦ç†: {str(e)}")
        return "ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒ¼ãƒ“ã‚¹è»¢é€ã‚µãƒ¼ãƒ“ã‚¹ã¯ä¸€æ™‚çš„ã«ã”åˆ©ç”¨ã„ãŸã ã‘ã¾ã›ã‚“ã€‚ã—ã°ã‚‰ãã—ã¦ã‹ã‚‰ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"

def show_model_config():
    """Display LLM configuration interface"""
    st.title("è¨€èªãƒ¢ãƒ‡ãƒ«è¨­å®š")

    #url setting
    st.text_input(
        "API URL",
        value = st.session_state.llm_config['url'],
        key="llm_url",
        help="è¨€èªãƒ¢ãƒ‡ãƒ«APIã®URLã‚¢ãƒ‰ãƒ¬ã‚¹"
    )

    #api key setting
    st.text_input(
        "API Key",
        value= st.session_state.llm_config['api_key'],
        type="password",
        key="llm_key",
        help="è¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹APIã‚­ãƒ¼"
    )

    #Model name
    st.text_input(
        "è¨€èªãƒ¢ãƒ‡ãƒ«å",
        value=st.session_state.llm_config['model'],
        key = "llm_model",
        help = "ä½¿ç”¨ã™ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã®åå‰ã‚’å…¥åŠ›ã—ã¾ã™ã€‚ä¾‹: deepseek-chat"
    )

    #Save button
    if st.button("ä¿å­˜", type="primary"):
        save_model_config()

    
def save_model_config():
    """è¨€èªãƒ¢ãƒ‡ãƒ«ã®è¨­å®š"""
    st.session_state.llm_config ={
        'url':st.session_state.llm_url,
        'api_key':st.session_state.llm_key,
        'model':st.session_state.llm_model
    }
    st.success("è¨­å®šãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ!")


def show_bot_config():
    """Display the customer service configuration interface"""
    st.title("AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆè¨­å®š")

    #Customer service name
    st.text_input(
        "AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆåå‰",
        value = st.session_state.bot_config['name'],
        key="bot_name",
        help="AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®åå‰ã‚’è¨­å®šã™ã‚‹"
    )

    #Customer service description
    st.text_area(
        "AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆèª¬æ˜",
        value=st.session_state.bot_config['description'],
        key="bot_description",
        help="AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®èª¬æ˜"
    )

    #Model
    st.text_input(
        "åˆ©ç”¨ãƒ¢ãƒ‡ãƒ«",
        value=st.session_state.llm_config['model'],
        disabled=True,
        help="ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆã®è¨€èªãƒ¢ãƒ‡ãƒ«"
    )

    if st.button("ä¿å­˜", type="primary"):
        save_bot_config()
        st.success("AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®è¨­å®šãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ")

def save_bot_config():
    st.session_state.bot_config={
        'name': st.session_state.bot_name,
        'description':st.session_state.bot_description
    }

def show_order_config():
    """Display the order configuration interface"""
    st.title("æ³¨æ–‡ç®¡ç†")
    
    # Add an order form
    with st.form("add_order_form"):
        st.subheader("æ–°ã—ã„æ³¨æ–‡ã‚’è¿½åŠ ")
        
        col1, col2 = st.columns(2)
        with col1:
            username = st.text_input("ãƒ¦ãƒ¼ã‚¶ãƒ¼å", key="order_username")
            product = st.text_input("è£½å“å", key="order_product")
        with col2:
            order_id = st.text_input("æ³¨æ–‡ç•ªå·", key="order_id")
            status = st.selectbox(
                "æ³¨æ–‡çŠ¶æ³",
                ["æ”¯æ‰•ã„å¾…ã¡", "æ”¯æ‰•ã„æ¸ˆã¿", "ç™ºé€æ¸ˆã¿", "å®Œäº†", "ã‚­ãƒ£ãƒ³ã‚»ãƒ«"],
                key="order_status"
            )
            date = st.date_input("æ—¥ä»˜", key="order_date")
        
        if st.form_submit_button("æ³¨æ–‡ã‚’è¿½åŠ ã™ã‚‹"):
            if not all([username, product, order_id]):
                st.error("å¿…é ˆé …ç›®ã‚’ã™ã¹ã¦å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                # Check if the order number already exists
                if any(order["order_id"] == order_id for order in st.session_state.orders):
                    st.error("æ³¨æ–‡ç•ªå·ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™")
                else:
                    # Add New Order
                    st.session_state.orders.append({
                        "username": username,
                        "product": product,
                        "order_id": order_id,
                        "status": status,
                        "date": str(date)
                    })
                    st.success("æ³¨æ–‡ãŒæ­£å¸¸ã«è¿½åŠ ã•ã‚Œã¾ã—ãŸ")
    
    # Display order list
    st.subheader("æ³¨æ–‡ãƒªã‚¹ãƒˆ")
    if st.session_state.orders:
        # Convert to DataFrame for better display
        orders_df = st.session_state.orders
        st.dataframe(orders_df)
    else:
        st.info("æ³¨æ–‡ãƒ‡ãƒ¼ã‚¿ã¯ã¾ã ã‚ã‚Šã¾ã›ã‚“")

def show_knowledge_config():
    """Display the knowledge base configuration interface"""
    st.title("ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®æ§‹æˆ")

    #Upload a TXT file
    st.title("ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    upload_file = st.file_uploader("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆTXTå½¢å¼ï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type="txt")

    #Parameter configuration
    st.title("ãƒ•ã‚¡ã‚¤ãƒ«ãƒ–ãƒ­ãƒƒã‚¯æ§‹æˆ")
    col1,col2 = st.columns(2)
    with col1:
        chunk_size = st.number_input("ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º", min_value=100, max_value=2000, value=100, step=50, help="å„ãƒ†ã‚­ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯ã®æœ€å¤§é•·ã‚’æŒ‡å®šã—ã¾ã™")
    with col2:
        chunk_overlap = st.number_input("ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚º", min_value=0, max_value=500, value=20, step=10, help="å„ãƒ†ã‚­ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—é•·ã‚’æŒ‡å®šã—ã¾ã™")

    #Delimiter configuration
    st.title("ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ãƒ¼ã®è¨­å®š")
    use_custom_separators = st.radio("ã‚«ã‚¹ã‚¿ãƒ åŒºåˆ‡ã‚Šæ–‡å­—ã®ä½¿ç”¨",["ã„ã„ãˆ","ã¯ã„"],index=0,help="åŒºåˆ‡ã‚Šæ–‡å­—ã«å¾“ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£ç¢ºã«åŒºåˆ‡ã‚‹ã«ã¯ã€ã€Œã¯ã„ã€ã‚’é¸æŠã—ã¾ã™")

    separators = None
    if use_custom_separators == "ã¯ã„":
        separators_input = st.text_input("åŒºåˆ‡ã‚Šæ–‡å­—å…¥åŠ›", value="###", help="ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒºåˆ‡ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ åŒºåˆ‡ã‚Šæ–‡å­—ã‚’å…¥åŠ›ã—ã¾ã™")
    
        separators = [ s.strip() for s in separators_input.split(",") if s.strip()]

    #Save uploaded files to session_state
    if upload_file is not None:
        st.session_state.knowledge_base['upload_file'] = upload_file
    #Process Document Button
    if st.button("æ–‡æ›¸å‡¦ç†", type="primary") and 'upload_file' in st.session_state.knowledge_base:
        with  st.spinner("æ–‡æ›¸å‡¦ç†ä¸­....."):
            #Process the document and embed it into the vector database. 
            #Return the 1st vector store: vector_store; the 2nd chunks: file chunks
            result= process_document_deepseek(
                st.session_state.knowledge_base['upload_file'],
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                custom_separators=(use_custom_separators == 'ã¯ã„'),
                separators= separators
            )
            
            st.session_state.knowledge_base['vector_store'] = result[0]
            st.session_state.knowledge_base['chunks'] = result[1]

    #Display file block
    if 'chunks' in st.session_state.knowledge_base:
        chunks = st.session_state.knowledge_base['chunks']
        st.subheader("æ–‡æ›¸ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœ")
        st.write(f"{len(chunks)} ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†ã‹ã‚Œã¦ã„ã¾ã™")

        #Show first 5 blocks
        for i, chunk in enumerate(chunks[:5],1):
            with st.expander(f"ãƒ†ã‚­ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯ #{i} (é•·ã•ï¼š{len(chunk)}æ–‡å­—)"):
                st.text(chunk)
        st.success("ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

    if st.session_state.knowledge_base['vector_store']:
        st.success("ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ãŒèª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸã€‚")    


def main():
    #1 Configuring Variables
    init_session_state()
    #2 Show left sidebar
    show_sidebar()
    #3 Show main window
    show_main_content()

if __name__ == "__main__":
    main()